[
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "flash_attn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flash_attn",
        "description": "flash_attn",
        "detail": "flash_attn",
        "documentation": {}
    },
    {
        "label": "flash_attn.layers.rotary",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flash_attn.layers.rotary",
        "description": "flash_attn.layers.rotary",
        "detail": "flash_attn.layers.rotary",
        "documentation": {}
    },
    {
        "label": "huggingface_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "omegaconf",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "omegaconf",
        "description": "omegaconf",
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "get_torch_dtype",
        "importPath": "utils.misc.get_torch_dtype",
        "description": "utils.misc.get_torch_dtype",
        "isExtraImport": true,
        "detail": "utils.misc.get_torch_dtype",
        "documentation": {}
    },
    {
        "label": "cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "selfies",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "selfies",
        "description": "selfies",
        "detail": "selfies",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "Fore",
        "importPath": "colorama",
        "description": "colorama",
        "isExtraImport": true,
        "detail": "colorama",
        "documentation": {}
    },
    {
        "label": "Style",
        "importPath": "colorama",
        "description": "colorama",
        "isExtraImport": true,
        "detail": "colorama",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "colorama",
        "description": "colorama",
        "isExtraImport": true,
        "detail": "colorama",
        "documentation": {}
    },
    {
        "label": "rank_zero_only",
        "importPath": "lightning.pytorch.utilities",
        "description": "lightning.pytorch.utilities",
        "isExtraImport": true,
        "detail": "lightning.pytorch.utilities",
        "documentation": {}
    },
    {
        "label": "hydra",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hydra",
        "description": "hydra",
        "detail": "hydra",
        "documentation": {}
    },
    {
        "label": "lightning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lightning",
        "description": "lightning",
        "detail": "lightning",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "hydra.utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hydra.utils",
        "description": "hydra.utils",
        "detail": "hydra.utils",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torchmetrics",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "transformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transformers",
        "description": "transformers",
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PreTrainedTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PreTrainedTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "utils.modeling.noise_schedule",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils.modeling.noise_schedule",
        "description": "utils.modeling.noise_schedule",
        "detail": "utils.modeling.noise_schedule",
        "documentation": {}
    },
    {
        "label": "utils.modeling.ema",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils.modeling.ema",
        "description": "utils.modeling.ema",
        "detail": "utils.modeling.ema",
        "documentation": {}
    },
    {
        "label": "utils.modeling.samplers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils.modeling.samplers",
        "description": "utils.modeling.samplers",
        "detail": "utils.modeling.samplers",
        "documentation": {}
    },
    {
        "label": "models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "models",
        "description": "models",
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "venv",
        "description": "venv",
        "isExtraImport": true,
        "detail": "venv",
        "documentation": {}
    },
    {
        "label": "preprocess_selfies_data",
        "importPath": "utils.misc.preprocess_data",
        "description": "utils.misc.preprocess_data",
        "isExtraImport": true,
        "detail": "utils.misc.preprocess_data",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "importPath": "tokenizer",
        "description": "tokenizer",
        "isExtraImport": true,
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "tokenize_selfies_vocab",
        "importPath": "tokenizer",
        "description": "tokenizer",
        "isExtraImport": true,
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "setup_training_logging",
        "importPath": "utils.misc.setup",
        "description": "utils.misc.setup",
        "isExtraImport": true,
        "detail": "utils.misc.setup",
        "documentation": {}
    },
    {
        "label": "resolve_paths",
        "importPath": "utils.misc.setup",
        "description": "utils.misc.setup",
        "isExtraImport": true,
        "detail": "utils.misc.setup",
        "documentation": {}
    },
    {
        "label": "print_batch",
        "importPath": "utils.misc.setup",
        "description": "utils.misc.setup",
        "isExtraImport": true,
        "detail": "utils.misc.setup",
        "documentation": {}
    },
    {
        "label": "get_dataloaders",
        "importPath": "utils.misc.create_datasets",
        "description": "utils.misc.create_datasets",
        "isExtraImport": true,
        "detail": "utils.misc.create_datasets",
        "documentation": {}
    },
    {
        "label": "fast_csv_to_df_reader",
        "importPath": "utils.misc.csv_data_reader",
        "description": "utils.misc.csv_data_reader",
        "isExtraImport": true,
        "detail": "utils.misc.csv_data_reader",
        "documentation": {}
    },
    {
        "label": "configure_logging",
        "importPath": "utils.misc.logging_config",
        "description": "utils.misc.logging_config",
        "isExtraImport": true,
        "detail": "utils.misc.logging_config",
        "documentation": {}
    },
    {
        "label": "decode",
        "importPath": "idna",
        "description": "idna",
        "isExtraImport": true,
        "detail": "idna",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "normalizers",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "pre_tokenizers",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "WordLevel",
        "importPath": "tokenizers.models",
        "description": "tokenizers.models",
        "isExtraImport": true,
        "detail": "tokenizers.models",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class Rotary(torch.nn.Module):\n  def __init__(self, dim, base=10_000):\n    super().__init__()\n    inv_freq = 1.0 / (\n      base ** (torch.arange(0, dim, 2).float() / dim)\n    )\n    self.register_buffer('inv_freq', inv_freq)\n    self.seq_len_cached = None\n    self.cos_cached = None\n    self.sin_cached = None",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class LayerNorm(nn.Module):\n  def __init__(self, dim):\n    super().__init__()\n    self.weight = nn.Parameter(torch.ones([dim]))\n    self.dim = dim\n  def forward(self, x):\n    with torch.cuda.amp.autocast(enabled=False):\n      x = F.layer_norm(x.float(), [self.dim])\n    return x * self.weight[None, None, :]\ndef residual_linear(x, W, x_skip, residual_scale):",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "DDiTBlock",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class DDiTBlock(nn.Module):\n  def __init__(\n    self,\n    dim,\n    n_heads,\n    cond_dim,\n    mlp_ratio=4,\n    dropout=0.1,\n    causal=False,\n  ):",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "EmbeddingLayer",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class EmbeddingLayer(nn.Module):\n  def __init__(self, dim, vocab_dim):\n    super().__init__()\n    self.embedding = nn.Parameter(\n      torch.empty((vocab_dim, dim))\n    )\n    torch.nn.init.kaiming_uniform_(\n      self.embedding, a=math.sqrt(5)\n    )\n  def forward(self, x):",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "DDitFinalLayer",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class DDitFinalLayer(nn.Module):\n  def __init__(\n    self, hidden_size, out_channels, cond_dim, causal=False\n  ):\n    super().__init__()\n    self.causal = causal\n    assert causal == True\n    self.norm_final = LayerNorm(hidden_size)\n    self.linear = nn.Linear(hidden_size, out_channels)\n    self.linear.weight.data.zero_()",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "DDIT",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class DDIT(nn.Module, huggingface_hub.PyTorchModelHubMixin):\n  def __init__(self, config, vocab_size: int):\n    super().__init__()\n    if type(config) == dict:\n      config = omegaconf.OmegaConf.create(config)\n    self.config = config\n    self.vocab_size = vocab_size\n    self.causal = (\n      hasattr(config.model, 'causal')\n      and config.model.causal",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "AR",
        "kind": 6,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "class AR(DDIT):\n  def __init__(self, config, vocab_size, mask_index):\n    super().__init__(config, vocab_size)\n    self.mask_index = mask_index\n    self.neg_infinity = -1000.0\n  def forward(self, xt, sigma):\n    \"\"\"Forward pass of the denoising model.\n    Args:\n      xt: int torch.Tensor with shape\n          (batch_size, diffusion_model_input_length), token ids.",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "bias_dropout_add_scale",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def bias_dropout_add_scale(\n  x: torch.Tensor,\n  bias: typing.Optional[torch.Tensor],\n  scale: torch.Tensor,\n  residual: typing.Optional[torch.Tensor],\n  prob: float,\n  training: bool,\n) -> torch.Tensor:\n  if bias is not None:\n    out = scale * F.dropout(",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "get_bias_dropout_add_scale",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def get_bias_dropout_add_scale(training):\n  def _bias_dropout_add(x, bias, scale, residual, prob):\n    return bias_dropout_add_scale(\n      x, bias, scale, residual, prob, training\n    )\n  return _bias_dropout_add\n@torch.jit.script\ndef bias_dropout_add_scale_fused_train(\n  x: torch.Tensor,\n  bias: typing.Optional[torch.Tensor],",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "bias_dropout_add_scale_fused_train",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def bias_dropout_add_scale_fused_train(\n  x: torch.Tensor,\n  bias: typing.Optional[torch.Tensor],\n  scale: torch.Tensor,\n  residual: typing.Optional[torch.Tensor],\n  prob: float,\n) -> torch.Tensor:\n  return bias_dropout_add_scale(\n    x, bias, scale, residual, prob, True\n  )",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "bias_dropout_add_scale_fused_inference",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def bias_dropout_add_scale_fused_inference(\n  x: torch.Tensor,\n  bias: typing.Optional[torch.Tensor],\n  scale: torch.Tensor,\n  residual: typing.Optional[torch.Tensor],\n  prob: float,\n) -> torch.Tensor:\n  return bias_dropout_add_scale(\n    x, bias, scale, residual, prob, False\n  )",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def rotate_half(x):\n  x1, x2 = (\n    x[..., : x.shape[-1] // 2],\n    x[..., x.shape[-1] // 2 :],\n  )\n  return torch.cat((-x2, x1), dim=-1)\ndef apply_rotary_pos_emb(qkv, cos, sin):\n  cos = cos[0, :, 0, 0, : cos.shape[-1] // 2]\n  sin = sin[0, :, 0, 0, : sin.shape[-1] // 2]\n  return flash_attn.layers.rotary.apply_rotary_emb_qkv_(",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def apply_rotary_pos_emb(qkv, cos, sin):\n  cos = cos[0, :, 0, 0, : cos.shape[-1] // 2]\n  sin = sin[0, :, 0, 0, : sin.shape[-1] // 2]\n  return flash_attn.layers.rotary.apply_rotary_emb_qkv_(\n    qkv, cos, sin\n  )\n#################################################################################\n#                                  Layers                                       #\n#################################################################################\nclass LayerNorm(nn.Module):",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "residual_linear",
        "kind": 2,
        "importPath": "models.autoregressive",
        "description": "models.autoregressive",
        "peekOfCode": "def residual_linear(x, W, x_skip, residual_scale):\n  \"\"\"x_skip + residual_scale * W @ x\"\"\"\n  dim_out, dim_in = W.shape[0], W.shape[1]\n  return torch.addmm(\n    x_skip.view(-1, dim_out),\n    x.view(-1, dim_in),\n    W.T,\n    alpha=residual_scale,\n  ).view(*x.shape[:-1], dim_out)\n#################################################################################",
        "detail": "models.autoregressive",
        "documentation": {}
    },
    {
        "label": "Rotary",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class Rotary(nn.Module):\n    def __init__(self, dim, base=10_000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n    def forward(self, x, seq_dim=1):\n        seq_len = x.shape[seq_dim]",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones([dim]))\n        self.dim = dim\n    def forward(self, x):\n        with torch.amp.autocast(device_type = x.device.type, enabled=False):\n            x = F.layer_norm(x.float(), [self.dim])\n        return x * self.weight[None, None, :]\ndef residual_linear(x, W, x_skip, residual_scale):",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "TimestepEmbedder",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class TimestepEmbedder(nn.Module):\n    \"\"\"\n    Embeds scalar timesteps into vector representations.\n    \"\"\"\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True)",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "LabelEmbedder",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class LabelEmbedder(nn.Module):\n    \"\"\"Embeds class labels into vector representations.\n    Also handles label dropout for classifier-free guidance.\n    \"\"\"\n    def __init__(self, num_classes, cond_size):\n        super().__init__()\n        self.embedding_table = nn.Embedding(num_classes + 1, cond_size)\n        self.num_classes = num_classes\n    def forward(self, labels):\n        return self.embedding_table(labels)",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "DDiTBlock",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class DDiTBlock(nn.Module):\n    def __init__(self, dim, n_heads, cond_dim, mlp_ratio=4, dropout=0.1):\n        super().__init__()\n        self.n_heads = n_heads\n        self.norm1 = LayerNorm(dim)\n        self.attn_qkv = nn.Linear(dim, 3 * dim, bias=False)\n        self.attn_out = nn.Linear(dim, dim, bias=False)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm2 = LayerNorm(dim)\n        self.mlp = nn.Sequential(",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "EmbeddingLayer",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class EmbeddingLayer(nn.Module):\n    def __init__(self, dim, vocab_dim):\n        super().__init__()\n        self.embedding = nn.Parameter(torch.empty((vocab_dim, dim)))\n        torch.nn.init.kaiming_uniform_(self.embedding, a=math.sqrt(5))\n    def forward(self, x):\n        return self.embedding[x]\nclass DDitFinalLayer(nn.Module):\n    def __init__(self, hidden_size, out_channels, cond_dim):\n        super().__init__()",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "DDitFinalLayer",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class DDitFinalLayer(nn.Module):\n    def __init__(self, hidden_size, out_channels, cond_dim):\n        super().__init__()\n        self.norm_final = LayerNorm(hidden_size)\n        self.linear = nn.Linear(hidden_size, out_channels)\n        self.linear.weight.data.zero_()\n        self.linear.bias.data.zero_()\n        self.adaLN_modulation = nn.Linear(cond_dim, 2 * hidden_size, bias=True)\n        self.adaLN_modulation.weight.data.zero_()\n        self.adaLN_modulation.bias.data.zero_()",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "DIT",
        "kind": 6,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "class DIT(nn.Module, huggingface_hub.PyTorchModelHubMixin):\n    def __init__(self, config, vocab_size: int):\n        super().__init__()\n        if isinstance(config, dict):\n            config = omegaconf.OmegaConf.create(config)\n        self.config = config\n        self.vocab_size = vocab_size\n        self.vocab_embed = EmbeddingLayer(config.model.hidden_size, vocab_size)\n        self.sigma_map = TimestepEmbedder(config.model.cond_dim)\n        self.rotary_emb = Rotary(config.model.hidden_size // config.model.n_heads)",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "bias_dropout_add_scale",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def bias_dropout_add_scale(\n    x: torch.Tensor,\n    bias: typing.Optional[torch.Tensor],\n    scale: torch.Tensor,\n    residual: typing.Optional[torch.Tensor],\n    prob: float,\n    training: bool) -> torch.Tensor:\n    if bias is not None:\n        out = scale * F.dropout(x + bias, p=prob, training=training)\n    else:",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "get_bias_dropout_add_scale",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def get_bias_dropout_add_scale(training):\n    def _bias_dropout_add(x, bias, scale, residual, prob):\n        return bias_dropout_add_scale(x, bias, scale, residual, prob, training)\n    return _bias_dropout_add\ndef modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n    return x * (1 + scale) + shift\n@torch.jit.script\ndef bias_dropout_add_scale_fused_train(\n    x: torch.Tensor,\n    bias: typing.Optional[torch.Tensor],",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "modulate",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def modulate(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n    return x * (1 + scale) + shift\n@torch.jit.script\ndef bias_dropout_add_scale_fused_train(\n    x: torch.Tensor,\n    bias: typing.Optional[torch.Tensor],\n    scale: torch.Tensor,\n    residual: typing.Optional[torch.Tensor],\n    prob: float) -> torch.Tensor:\n    return bias_dropout_add_scale(x, bias, scale, residual, prob, True)",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "bias_dropout_add_scale_fused_train",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def bias_dropout_add_scale_fused_train(\n    x: torch.Tensor,\n    bias: typing.Optional[torch.Tensor],\n    scale: torch.Tensor,\n    residual: typing.Optional[torch.Tensor],\n    prob: float) -> torch.Tensor:\n    return bias_dropout_add_scale(x, bias, scale, residual, prob, True)\n@torch.jit.script\ndef bias_dropout_add_scale_fused_inference(\n    x: torch.Tensor,",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "bias_dropout_add_scale_fused_inference",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def bias_dropout_add_scale_fused_inference(\n    x: torch.Tensor,\n    bias: typing.Optional[torch.Tensor],\n    scale: torch.Tensor,\n    residual: typing.Optional[torch.Tensor],\n    prob: float) -> torch.Tensor:\n    return bias_dropout_add_scale(x, bias, scale, residual, prob, False)\n@torch.jit.script\ndef modulate_fused(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n    return modulate(x, shift, scale)",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "modulate_fused",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def modulate_fused(x: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n    return modulate(x, shift, scale)\nclass Rotary(nn.Module):\n    def __init__(self, dim, base=10_000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def rotate_half(x):\n    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\ndef apply_rotary_pos_emb(qkv, cos, sin):\n    if USE_FLASH_ATTN:\n        # Use flash_attn's rotary function\n        cos_ = cos[0, :, 0, 0, :cos.shape[-1] // 2]\n        sin_ = sin[0, :, 0, 0, :sin.shape[-1] // 2]\n        return flash_attn.layers.rotary.apply_rotary_emb_qkv_(qkv, cos_, sin_)\n    else:",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def apply_rotary_pos_emb(qkv, cos, sin):\n    if USE_FLASH_ATTN:\n        # Use flash_attn's rotary function\n        cos_ = cos[0, :, 0, 0, :cos.shape[-1] // 2]\n        sin_ = sin[0, :, 0, 0, :sin.shape[-1] // 2]\n        return flash_attn.layers.rotary.apply_rotary_emb_qkv_(qkv, cos_, sin_)\n    else:\n        # Custom implementation for Windows\n        # Assume qkv shape: [b, s, three, h, d]\n        d_rot = qkv.shape[-1] // 2",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "modulate",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def modulate(x, shift, scale):\n    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n#################################################################################\n#                                  Layers                                       #\n#################################################################################\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones([dim]))\n        self.dim = dim",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "residual_linear",
        "kind": 2,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "def residual_linear(x, W, x_skip, residual_scale):\n    \"\"\"x_skip + residual_scale * W @ x\"\"\"\n    dim_out, dim_in = W.shape[0], W.shape[1]\n    return torch.addmm(x_skip.view(-1, dim_out),\n                       x.view(-1, dim_in),\n                       W.T,\n                       alpha=residual_scale).view(*x.shape[:-1], dim_out)\n#################################################################################\n#               Embedding Layers for Timesteps and Class Labels                 #\n#################################################################################",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "models.dit",
        "description": "models.dit",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Detect OS and import appropriate efficient attention library.\nif sys.platform.startswith('win'):\n    USE_FLASH_ATTN = False\nelse:\n    try:\n        import flash_attn\n        import flash_attn.layers.rotary\n        USE_FLASH_ATTN = True\n    except ImportError:",
        "detail": "models.dit",
        "documentation": {}
    },
    {
        "label": "check_gpu_compatibility",
        "kind": 2,
        "importPath": "utils.misc.create_datasets",
        "description": "utils.misc.create_datasets",
        "peekOfCode": "def check_gpu_compatibility(config):\n    logger.info(\"Checking GPU compatibility\")\n    num_gpus = torch.cuda.device_count()\n    assert (config.loader.global_batch_size == (config.loader.batch_size\n                                              * config.er.num_nodes\n                                              * num_gpus\n                                              * conficonfig.cumulate_grad_batches))\n    if config.loader.global_batch_size % (\n        num_gpus * config.traconfig.ate_grad_batches) != 0:\n        raise ValueError(",
        "detail": "utils.misc.create_datasets",
        "documentation": {}
    },
    {
        "label": "create_train_val_dataloaders",
        "kind": 2,
        "importPath": "utils.misc.create_datasets",
        "description": "utils.misc.create_datasets",
        "peekOfCode": "def create_train_val_dataloaders(config, tokenized_selfies_data, pin_memory=True, valid_seed=None):\n    # Create a Hugging Face Dataset\n    dataset = datasets.Dataset.from_dict(tokenized_selfies_data)\n    # Potentially also include the column \"conditioning\". First requires adding that to the tokenized selfies data\n    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\n    # Split the data\n    split_dataset = dataset.train_test_split(test_size=1 - config.train_test_split.train)\n    train_dataset = split_dataset['train']\n    val_dataset = split_dataset['test']\n    # Create PyTorch DataLoaders for training and validation.",
        "detail": "utils.misc.create_datasets",
        "documentation": {}
    },
    {
        "label": "get_dataloaders",
        "kind": 2,
        "importPath": "utils.misc.create_datasets",
        "description": "utils.misc.create_datasets",
        "peekOfCode": "def get_dataloaders(config,tokenized_selfies_data, tokenizer):\n    check_gpu_compatibility(config)\n    train_set, valid_set = create_train_val_dataloaders(config, tokenized_selfies_data)\n    return train_set, valid_set",
        "detail": "utils.misc.create_datasets",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "utils.misc.create_datasets",
        "description": "utils.misc.create_datasets",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef check_gpu_compatibility(config):\n    logger.info(\"Checking GPU compatibility\")\n    num_gpus = torch.cuda.device_count()\n    assert (config.loader.global_batch_size == (config.loader.batch_size\n                                              * config.er.num_nodes\n                                              * num_gpus\n                                              * conficonfig.cumulate_grad_batches))\n    if config.loader.global_batch_size % (\n        num_gpus * config.traconfig.ate_grad_batches) != 0:",
        "detail": "utils.misc.create_datasets",
        "documentation": {}
    },
    {
        "label": "fast_csv_to_df_reader",
        "kind": 2,
        "importPath": "utils.misc.csv_data_reader",
        "description": "utils.misc.csv_data_reader",
        "peekOfCode": "def fast_csv_to_df_reader(file_path: str, row_limit: int = 10) -> pd.Series:\n    \"\"\"\n    Gets the column names from a large CSV file and then loads the rest of the data.\n    Args:\n        file_path (str): Path to the CSV file.\n        row_limit (int): Maximum number of rows to read excluding column headers(default 10).\n    Returns:\n        pd.df: Dataframe with the column names and the first 10 rows of data. Exclude the column names from the data.\n    \"\"\"\n    # Read only the header to get column names",
        "detail": "utils.misc.csv_data_reader",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "utils.misc.csv_data_reader",
        "description": "utils.misc.csv_data_reader",
        "peekOfCode": "logger = logging.getLogger(__name__)\nimport selfies as sf\ndef fast_csv_to_df_reader(file_path: str, row_limit: int = 10) -> pd.Series:\n    \"\"\"\n    Gets the column names from a large CSV file and then loads the rest of the data.\n    Args:\n        file_path (str): Path to the CSV file.\n        row_limit (int): Maximum number of rows to read excluding column headers(default 10).\n    Returns:\n        pd.df: Dataframe with the column names and the first 10 rows of data. Exclude the column names from the data.",
        "detail": "utils.misc.csv_data_reader",
        "documentation": {}
    },
    {
        "label": "get_torch_dtype",
        "kind": 2,
        "importPath": "utils.misc.get_torch_dtype",
        "description": "utils.misc.get_torch_dtype",
        "peekOfCode": "def get_torch_dtype(precision):\n    \"\"\"Converts precision string from config to PyTorch dtype.\"\"\"\n    mapping = {\n        \"16-mixed\": torch.float16,\n        \"bf16\": torch.bfloat16,\n        \"32\": torch.float32,  # Default to full precision\n    }\n    return mapping.get(precision, torch.float32)  # Defaults to float32 if unknown",
        "detail": "utils.misc.get_torch_dtype",
        "documentation": {}
    },
    {
        "label": "ColoredFormatter",
        "kind": 6,
        "importPath": "utils.misc.logging_config",
        "description": "utils.misc.logging_config",
        "peekOfCode": "class ColoredFormatter(logging.Formatter):\n    COLORS = {\n        'DEBUG': Fore.CYAN,\n        'INFO': Fore.GREEN,\n        'WARNING': Fore.YELLOW,\n        'ERROR': Fore.RED,\n        'CRITICAL': Fore.MAGENTA + Style.BRIGHT\n    }\n    def format(self, record):\n        log_message = super().format(record)",
        "detail": "utils.misc.logging_config",
        "documentation": {}
    },
    {
        "label": "configure_logging",
        "kind": 2,
        "importPath": "utils.misc.logging_config",
        "description": "utils.misc.logging_config",
        "peekOfCode": "def configure_logging():\n    colored_formatter = ColoredFormatter(\n        fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n    # Create a file handler (without colors)\n    file_handler = logging.FileHandler('app.log')\n    file_handler.setFormatter(logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    ))",
        "detail": "utils.misc.logging_config",
        "documentation": {}
    },
    {
        "label": "preprocess_selfies_data",
        "kind": 2,
        "importPath": "utils.misc.preprocess_data",
        "description": "utils.misc.preprocess_data",
        "peekOfCode": "def preprocess_selfies_data(raw_data):\n    \"\"\" This function preprocesses raw data by \n    splitting the selfies data and returning the alphabet of the selfies data\"\"\"\n    if 'selfies' not in raw_data.columns:\n        logger.warning(\"'selfies' column not found in raw_data. Cannot create vocabulary.\")\n        exit()\n    else:\n        logger.info(\"'selfies' column found. Creating vocabulary from SELFIES data...\")\n        alphabet = selfies.get_alphabet_from_selfies(raw_data['selfies'])\n        tokenized_list_col = []",
        "detail": "utils.misc.preprocess_data",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "utils.misc.preprocess_data",
        "description": "utils.misc.preprocess_data",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef preprocess_selfies_data(raw_data):\n    \"\"\" This function preprocesses raw data by \n    splitting the selfies data and returning the alphabet of the selfies data\"\"\"\n    if 'selfies' not in raw_data.columns:\n        logger.warning(\"'selfies' column not found in raw_data. Cannot create vocabulary.\")\n        exit()\n    else:\n        logger.info(\"'selfies' column found. Creating vocabulary from SELFIES data...\")\n        alphabet = selfies.get_alphabet_from_selfies(raw_data['selfies'])",
        "detail": "utils.misc.preprocess_data",
        "documentation": {}
    },
    {
        "label": "resolve_paths",
        "kind": 2,
        "importPath": "utils.misc.setup",
        "description": "utils.misc.setup",
        "peekOfCode": "def resolve_paths(config: DictConfig):\n    \"\"\"\n    Recursively resolves all paths in a Hydra configuration\n    while ignoring non-path fields like metric names.\n    \"\"\"\n    def _resolve(obj, key=None):\n        # Skip resolving 'monitor' or other non-path fields\n        if key in [\"monitor\", \"id\", \"tags\"]:\n            return obj  # Do not modify metric names\n        # If it's a string and looks like a path, resolve it",
        "detail": "utils.misc.setup",
        "documentation": {}
    },
    {
        "label": "setup_training_logging",
        "kind": 2,
        "importPath": "utils.misc.setup",
        "description": "utils.misc.setup",
        "peekOfCode": "def setup_training_logging(config):\n    \"\"\"Sets up wandb logging for training. Also checks for any checkpoints to resume from and implements callbacks\"\"\"\n    wandb_logger = None\n    if config.get('wandb', None) is not None:\n        wandb_logger = L.pytorch.loggers.WandbLogger(\n        config=OmegaConf.to_object(config),\n        ** config.wandb)\n    if (config.checkpointing.resume_from_ckpt\n        and config.checkpointing.resume_ckpt_path is not None\n        ):",
        "detail": "utils.misc.setup",
        "documentation": {}
    },
    {
        "label": "print_batch",
        "kind": 2,
        "importPath": "utils.misc.setup",
        "description": "utils.misc.setup",
        "peekOfCode": "def print_batch(train_ds, valid_ds, tokenizer, k=8):\n  for dl_type, dl in [\n    ('train', train_ds), ('valid', valid_ds)]:\n    print(f'Printing {dl_type} dataloader batch.')\n    batch = next(iter(dl))\n    print('Batch input_ids.shape', batch['input_ids'].shape)\n    first = batch['input_ids'][0, :k]\n    last = batch['input_ids'][0, -k:]\n    print(f'First {k} tokens:', tokenizer.decode(first, skip_special_tokens=True))\n    print('ids:', first)",
        "detail": "utils.misc.setup",
        "documentation": {}
    },
    {
        "label": "ExponentialMovingAverage",
        "kind": 6,
        "importPath": "utils.modeling.ema",
        "description": "utils.modeling.ema",
        "peekOfCode": "class ExponentialMovingAverage:\n  \"\"\"\n  Maintains (exponential) moving average of a set of parameters.\n  \"\"\"\n  def __init__(self, parameters, decay, use_num_updates=True):\n    \"\"\"\n    Args:\n        parameters: Iterable of `torch.nn.Parameter`; usually the result of\n            `model.parameters()`.\n        decay: The exponential decay.",
        "detail": "utils.modeling.ema",
        "documentation": {}
    },
    {
        "label": "Noise",
        "kind": 6,
        "importPath": "utils.modeling.noise_schedule",
        "description": "utils.modeling.noise_schedule",
        "peekOfCode": "class Noise(abc.ABC, nn.Module):\n  \"\"\"\n  Baseline forward method to get the total + rate of noise at a timestep\n  \"\"\"\n  def forward(self, t):\n    # Assume time goes from 0 to 1\n    return self.total_noise(t), self.rate_noise(t)\n  @abc.abstractmethod\n  def rate_noise(self, t):\n    \"\"\"",
        "detail": "utils.modeling.noise_schedule",
        "documentation": {}
    },
    {
        "label": "Linear",
        "kind": 6,
        "importPath": "utils.modeling.noise_schedule",
        "description": "utils.modeling.noise_schedule",
        "peekOfCode": "class Linear(Noise):\n  def __init__(self, sigma_min=0, sigma_max=10, dtype=torch.float32):\n    super().__init__()\n    self.sigma_min = torch.tensor(sigma_min, dtype=dtype)\n    self.sigma_max = torch.tensor(sigma_max, dtype=dtype)\n  def rate_noise(self, t):\n    return self.sigma_max - self.sigma_min\n  def total_noise(self, t):\n    return self.sigma_min + t * (self.sigma_max - self.sigma_min)\n  def importance_sampling_transformation(self, t):",
        "detail": "utils.modeling.noise_schedule",
        "documentation": {}
    },
    {
        "label": "LogLinearNoise",
        "kind": 6,
        "importPath": "utils.modeling.noise_schedule",
        "description": "utils.modeling.noise_schedule",
        "peekOfCode": "class LogLinearNoise(Noise):\n  \"\"\"Log Linear noise schedule.\n  Built such that 1 - 1/e^(n(t)) interpolates between 0 and\n  ~1 when t varies from 0 to 1. Total noise is\n  -log(1 - (1 - eps) * t), so the sigma will be\n  (1 - eps) * t.\n  \"\"\"\n  def __init__(self, eps=1e-3):\n    super().__init__()\n    self.eps = eps",
        "detail": "utils.modeling.noise_schedule",
        "documentation": {}
    },
    {
        "label": "get_noise",
        "kind": 2,
        "importPath": "utils.modeling.noise_schedule",
        "description": "utils.modeling.noise_schedule",
        "peekOfCode": "def get_noise(config, dtype=torch.float32):\n  if config.noise.type == 'loglinear':\n    return LogLinearNoise()\n  elif config.noise.type == 'linear':\n    return Linear(config.noise.sigma_min,\n                  config.noise.sigma_max,\n                  dtype)\n  else:\n    raise ValueError(f'{config.noise.type} is not a valid noise')\ndef binary_discretization(z):",
        "detail": "utils.modeling.noise_schedule",
        "documentation": {}
    },
    {
        "label": "binary_discretization",
        "kind": 2,
        "importPath": "utils.modeling.noise_schedule",
        "description": "utils.modeling.noise_schedule",
        "peekOfCode": "def binary_discretization(z):\n  z_hard = torch.sign(z)\n  z_soft = z / torch.norm(z, dim=-1, keepdim=True)\n  return z_soft + (z_hard - z_soft).detach()\nclass Noise(abc.ABC, nn.Module):\n  \"\"\"\n  Baseline forward method to get the total + rate of noise at a timestep\n  \"\"\"\n  def forward(self, t):\n    # Assume time goes from 0 to 1",
        "detail": "utils.modeling.noise_schedule",
        "documentation": {}
    },
    {
        "label": "RandomFaultTolerantSampler",
        "kind": 6,
        "importPath": "utils.modeling.samplers",
        "description": "utils.modeling.samplers",
        "peekOfCode": "class RandomFaultTolerantSampler(RandomSampler):\n    def __init__(self, *args, generator=None, **kwargs):\n        if generator is None:\n            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n            generator = torch.Generator().manual_seed(seed)\n        kwargs.pop('shuffle', None)\n        super().__init__(*args, generator=generator, **kwargs)\n        self.counter = 0\n        self.restarting = False\n        self.state = self.generator.get_state()  # <-- FIX: Initialize self.state",
        "detail": "utils.modeling.samplers",
        "documentation": {}
    },
    {
        "label": "FaultTolerantDistributedSampler",
        "kind": 6,
        "importPath": "utils.modeling.samplers",
        "description": "utils.modeling.samplers",
        "peekOfCode": "class FaultTolerantDistributedSampler(DistributedSampler):\n    \"\"\"\n    A distributed sampler that supports fault tolerance by tracking sampling progress.\n    Ensures consistent sample distribution across multiple GPUs/nodes.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.counter = 0\n        self.restarting = False\n        self.state = None  # Add this if needed for consistency",
        "detail": "utils.modeling.samplers",
        "documentation": {}
    },
    {
        "label": "Loss",
        "kind": 6,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "class Loss:\n  loss: torch.FloatTensor\n  nlls: torch.FloatTensor\n  token_mask: torch.FloatTensor\nclass NLL(torchmetrics.aggregation.MeanMetric):\n  pass\nclass BPD(NLL):\n  def compute(self) -> Tensor:\n    \"\"\"Computes the bits per dimension.\n    Returns:",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "NLL",
        "kind": 6,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "class NLL(torchmetrics.aggregation.MeanMetric):\n  pass\nclass BPD(NLL):\n  def compute(self) -> Tensor:\n    \"\"\"Computes the bits per dimension.\n    Returns:\n      bpd\n    \"\"\"\n    return self.mean_value / self.weight / LOG2\nclass Perplexity(NLL):",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "BPD",
        "kind": 6,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "class BPD(NLL):\n  def compute(self) -> Tensor:\n    \"\"\"Computes the bits per dimension.\n    Returns:\n      bpd\n    \"\"\"\n    return self.mean_value / self.weight / LOG2\nclass Perplexity(NLL):\n  def compute(self) -> Tensor:\n    \"\"\"Computes the Perplexity.",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "Perplexity",
        "kind": 6,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "class Perplexity(NLL):\n  def compute(self) -> Tensor:\n    \"\"\"Computes the Perplexity.\n    Returns:\n     Perplexity\n    \"\"\"\n    return torch.exp(self.mean_value / self.weight)\nclass Diffusion(L.LightningModule):\n  def __init__(\n    self,",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "Diffusion",
        "kind": 6,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "class Diffusion(L.LightningModule):\n  def __init__(\n    self,\n    config,\n    tokenizer: transformers.PreTrainedTokenizer):\n    super().__init__()\n    self.save_hyperparameters()\n    self.config = config\n    self.tokenizer = tokenizer\n    self.vocab_size = self.tokenizer.vocab_size",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "logger = logging.getLogger(__name__)\nLOG2 = math.log(2)\ndef _sample_categorical(categorical_probs):\n  gumbel_norm = (\n    1e-10\n    - (torch.rand_like(categorical_probs) + 1e-10).log())\n  return (categorical_probs / gumbel_norm).argmax(dim=-1)\ndef _unsqueeze(x, reference):\n  return x.view(\n    * x.shape,",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "LOG2",
        "kind": 5,
        "importPath": "diffusion",
        "description": "diffusion",
        "peekOfCode": "LOG2 = math.log(2)\ndef _sample_categorical(categorical_probs):\n  gumbel_norm = (\n    1e-10\n    - (torch.rand_like(categorical_probs) + 1e-10).log())\n  return (categorical_probs / gumbel_norm).argmax(dim=-1)\ndef _unsqueeze(x, reference):\n  return x.view(\n    * x.shape,\n    * ((1,) * (len(reference.shape) - len(x.shape))))",
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def run(config: DictConfig):\n    # log config settings\n    logger.info(OmegaConf.to_yaml(config))\n    L.seed_everything(config.seed, verbose=False)\n    config = resolve_paths(config)\n    raw_data = fast_csv_to_df_reader(config.directory_paths.raw_data, row_limit=10)\n    # Data  goes from \"[C][=C][C]\" to ['[C]', '[=C]', '[C]'] and obtain alphabet\n    selfies_vocab, data = preprocess_selfies_data(raw_data)\n    # Passes selfies_vocab in case the tokenizer needs to be trained.\n    tokenizer = get_tokenizer(config, selfies_vocab)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef _train(config, tokenizer, data):\n    import diffusion\n    logger.info('Starting Training.')\n    wandb_logger, ckpt_path, callbacks = setup_training_logging(config)\n    tokenized_data, vocab_size = tokenize_selfies_vocab(tokenizer, config, data)\n    train_dataloader, val_dataloader = get_dataloaders(config, tokenized_data, tokenizer)\n    model = diffusion.Diffusion(config, tokenizer=tokenizer)\n  # print_batch(train_dataloader, val_dataloader, tokenizer) # takes a a long time so only run if necessary.\n    trainer = hydra.utils.instantiate(",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "SelfiesTokenizer",
        "kind": 6,
        "importPath": "tokenizer",
        "description": "tokenizer",
        "peekOfCode": "class SelfiesTokenizer(PreTrainedTokenizerFast):\n    \"\"\"\n    A custom tokenizer that inherits from PreTrainedTokenizerFast and\n    builds a vocabulary from a SELFIES alphabet plus special tokens.\n    \"\"\"\n    def __init__(\n        self,\n        selfies_vocab: Set[str] = None,\n        bos_token=\"[BOS]\",\n        eos_token=\"[EOS]\",",
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "get_tokenizer",
        "kind": 2,
        "importPath": "tokenizer",
        "description": "tokenizer",
        "peekOfCode": "def get_tokenizer(config, selfies_vocab):\n    if os.path.exists(config.directory_paths.tokenizer):\n        logger.info(\"Tokenizer folder found. Loading...\")\n        try:\n            tokenizer = SelfiesTokenizer.from_pretrained(config.directory_paths.tokenizer)\n        except Exception as e:\n            logger.error(f\"Error loading tokenizer: {e}\")\n            exit()\n    else:\n        logger.info(f\"No tokenizer found at {config.directory_paths.tokenizer}. Creating...\")",
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "add_bos_and_eos_tokens",
        "kind": 2,
        "importPath": "tokenizer",
        "description": "tokenizer",
        "peekOfCode": "def add_bos_and_eos_tokens(tokenized_data, tokenizer):\n    \"\"\"\n    Adds BOS and EOS tokens to each sequence in the BatchEncoding.\n    Updates both 'input_ids' AND 'attention_mask' to maintain alignment.\n    Assumes:\n      - tokenized_data['input_ids'] is a torch.Tensor of shape (batch_size, seq_length).\n      - tokenized_data['attention_mask'] is a torch.Tensor of shape (batch_size, seq_length).\n      - The user wants to prepend BOS and append EOS tokens to input_ids, plus reflect that in attention_mask.\n    \"\"\"\n    bos_id = tokenizer.bos_token_id",
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "tokenize_selfies_vocab",
        "kind": 2,
        "importPath": "tokenizer",
        "description": "tokenizer",
        "peekOfCode": "def tokenize_selfies_vocab(tokenizer, config, raw_data):\n    # If encoded inputs are not found, tokenize the SELFIES data, else, load the tokenized data\n    if os.path.exists(config.directory_paths.train_data_encoding):\n        logger.info(f\"SELFIES training data encoding found, loading data from {config.directory_paths.train_data_encoding}\")\n        try :\n            tokenized_data = torch.load(config.directory_paths.train_data_encoding, weights_only=False)\n            logger.info(f\"SELFIES data loaded successfully. Vocabulary size is: {tokenizer.vocab_size}\")\n            return tokenized_data, tokenizer.vocab_size\n        except Exception as e:\n            logger.error(f\"Error loading SELFIES data: {e}\")",
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "tokenizer",
        "description": "tokenizer",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass SelfiesTokenizer(PreTrainedTokenizerFast):\n    \"\"\"\n    A custom tokenizer that inherits from PreTrainedTokenizerFast and\n    builds a vocabulary from a SELFIES alphabet plus special tokens.\n    \"\"\"\n    def __init__(\n        self,\n        selfies_vocab: Set[str] = None,\n        bos_token=\"[BOS]\",",
        "detail": "tokenizer",
        "documentation": {}
    }
]