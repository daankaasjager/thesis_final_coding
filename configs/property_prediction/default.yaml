# configs/property_prediction/default.yaml

# This file contains everything needed for the property prediction task.

data:
  path: '/scratch/s3905845/thesis_final_coding/data/kraken/training_data/descriptors_v3.csv'
  prop_columns: ['nbo_P', 'nmr_P', 'pyr_P', 'fmo_mu', 'vmin_r', 'volume', 'fmo_eta',  'fukui_m', 'fukui_p', 'nuesp_P', 'somo_rc', 'nbo_P_rc', 'pyr_alpha', 'qpole_amp', 'vbur_vbur', 'Pint_P_min', 'sterimol_L', 'sterimol_B1', 'sterimol_B5', 'dipolemoment', 'efgtens_xx_P',  'efgtens_yy_P', 'nbo_bd_e_max', 'nbo_lp_P_occ', 'qpoletens_yy', 'E_solv_elstat', 'nbo_bds_e_avg', 'sterimol_burL', 'nbo_bd_occ_avg', 'sterimol_burB5', 'vbur_ovbur_min', 'vbur_qvbur_min', 'nbo_bds_occ_max', 'vbur_ratio_vbur_vtot', 'mol_wt', 'sa_score']

experiment:
  name: 'model_size_tiny' 

model:
  node_dim: null      # Set automatically by the script
  edge_dim: null      # Set automatically by the script
  out_dim: null       # Set automatically by the script
  atom_dim: 128
  num_embeds: 3
  emb_steps: 3
  conv:
    dim: 64
    aggr: 'mean'
  mlp:
    layers: [256, 128]
    batch_norm: True
    dropout: 0.25
  optimizer:
    class_path: 'torch.optim.Adam'
    init_args: {'lr': 1e-4}
  lr_scheduler:
    class_path: 'torch.optim.lr_scheduler.ReduceLROnPlateau'
    init_args: {'mode': 'min', 'factor': 0.5, 'patience': 10}

training:
  output_dir: '/scratch/s3905845/thesis_final_coding/checkpoints/property_prediction'
  batch_size: 64
  val_ratio: 0.20
  num_workers: 8
  precision: 'bf16-mixed' # BF16 float precision is better. CHANGE FOR LINUX
  max_epochs: 200
  accelerator: 'auto'
  devices: 'auto'
  early_stopping_patience: 25

wandb:
  project: SELFIES-diffusion
  notes: "property prediction training"
  group: property_prediction_training # Group runs by experiment name
  name: property_prediction # A descriptive name
  job_type: ${mode} # The type of job, e.g., train, test, etc.
  id: ${.name}_${seed}_${now:%Y%m%d_%H%M%S} #Sets unique name and seed

callbacks:
  checkpoint_every_n_steps:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: -1 # Do not save any "best" models; this callback is being used to save every n train steps
    save_last: True # save model as ${save_dir}/checkpoints/last.ckpt
    dirpath: ${cwd:}/checkpoints/property_prediction
    verbose: True
    auto_insert_metric_name: False
    every_n_train_steps: 4000
  checkpoint_monitor:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/loss # name of the logged metric which determines when model is improving
    mode: min # can be "max" or "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: False # True = additionally always save model from last epoch
    dirpath: ${cwd:}/checkpoints
    filename: property_prediction
    auto_insert_metric_name: False
    verbose: True
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step


# Section for inference settings
inference:
  model_path: '${cwd:}/checkpoints/property_prediction.ckpt' # Note the '..' to go up one level
  plotting_path: /scratch/s3905845/thesis_final_coding/data/kraken/metric_plots
  sampled_selfies_file: /scratch/s3905845/thesis_final_coding/data/kraken/sampled_data/${experiment.name}/generated_samples.json
  hist_sampled_selfies_file: /scratch/s3905845/thesis_final_coding/data/kraken/sampled_data/${experiment.name}/hist_generated_samples.json
  hist: True
  normalization_stats_file: "/scratch/s3905845/thesis_final_coding/data/kraken/mapping/mean_std.json"
  output_path: /scratch/s3905845/thesis_final_coding/data/kraken/sampled_data/${experiment.name}/samples_with_properties.json
  hist_output_path: /scratch/s3905845/thesis_final_coding/data/kraken/sampled_data/${experiment.name}/hist_samples_with_properties.json
  selfies_alphabet: /scratch/s3905845/thesis_final_coding/data/kraken/training_data/selfies_alphabet.txt
