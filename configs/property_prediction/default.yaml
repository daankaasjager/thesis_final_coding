# configs/property_prediction/default.yaml

# This file contains everything needed for the property prediction task.

data:
  path: '/scratch/s3905845/thesis_final_coding/data/kraken/training_data/descriptors_v3.csv'
  prop_columns: ['nbo_P', 'nmr_P', 'pyr_P', 'fmo_mu', 'vmin_r', 'volume', 'fmo_eta',  'fukui_m', 'fukui_p', 'nuesp_P', 'somo_rc', 'nbo_P_rc', 'pyr_alpha', 'qpole_amp', 'vbur_vbur', 'Pint_P_min', 'sterimol_L', 'sterimol_B1', 'sterimol_B5', 'dipolemoment', 'efgtens_xx_P',  'efgtens_yy_P', 'nbo_bd_e_max', 'nbo_lp_P_occ', 'qpoletens_yy', 'E_solv_elstat', 'nbo_bds_e_avg', 'sterimol_burL', 'nbo_bd_occ_avg', 'sterimol_burB5', 'vbur_ovbur_min', 'vbur_qvbur_min', 'nbo_bds_occ_max', 'vbur_ratio_vbur_vtot', 'mol_wt', 'sa_score']

experiment:
  name: 'model_size_tiny' 

model:
  atom_dim: 64  # Voinea uses 64 input dim, not 128
  num_embeds: 3
  emb_steps: 3
  conv:
    dim: 64
    aggr: 'mean'
  mlp:
    layers: [128, 64, 128]  # Match Voinea exactly
    batch_norm: True
    dropout: 0.5 
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 3e-3
      weight_decay: 1e-4
  lr_scheduler:
    class_path: 'torch.optim.lr_scheduler.CosineAnnealingLR'
    init_args: {'T_max': 250}  # T_max typically set to total epochs
  reg:
    edge_dropout: 0.1     # edge dropout in graph
    node_dropout: 0.05    # feature dropout before GRU

training:
  batch_size: 128 
  max_epochs: 250  
  val_ratio: 0.20
  num_workers: 8
  precision: 'bf16-mixed'
  accelerator: 'auto'
  devices: 'auto'
  early_stopping_patience: 25


wandb:
  project: SELFIES-diffusion
  notes: "property prediction training"
  group: property_prediction_training # Group runs by experiment name
  name: property_prediction # A descriptive name
  job_type: ${mode} # The type of job, e.g., train, test, etc.
  id: ${.name}_${seed}_${now:%Y%m%d_%H%M%S} #Sets unique name and seed

callbacks:
  checkpoint_every_n_steps:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: -1 # Do not save any "best" models; this callback is being used to save every n train steps
    save_last: True # save model as ${save_dir}/checkpoints/last.ckpt
    dirpath: ${cwd:}/checkpoints/property_prediction
    verbose: True
    auto_insert_metric_name: False
    every_n_train_steps: 4000
  checkpoint_monitor:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/loss # name of the logged metric which determines when model is improving
    mode: min # can be "max" or "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: False # True = additionally always save model from last epoch
    dirpath: ${cwd:}/checkpoints
    filename: property_prediction
    auto_insert_metric_name: False
    verbose: True
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    patience: ${property_prediction.training.early_stopping_patience}
    mode: min

sample_dir: /scratch/s3905845/thesis_final_coding/data/kraken/sampled_data/
# Section for inference settings
inference:
  model_path: '${cwd:}/checkpoints/property_prediction.ckpt' # Note the '..' to go up one level
  plotting_path: /scratch/s3905845/thesis_final_coding/data/kraken/metric_plots
  sampled_selfies_file: ${property_prediction.sample_dir}${experiment.name}/generated_samples.json
  hist_sampled_selfies_file: ${property_prediction.sample_dir}${experiment.name}/hist_generated_samples.json
  hist: True
  output_path: ${property_prediction.sample_dir}${experiment.name}/samples_with_properties.json
  hist_output_path: ${property_prediction.sample_dir}${experiment.name}/hist_samples_with_properties.json
  selfies_alphabet: /scratch/s3905845/thesis_final_coding/data/kraken/training_data/selfies_alphabet.txt
  normalization_stats_file: "/scratch/s3905845/thesis_final_coding/data/kraken/mapping/data_info.json"
