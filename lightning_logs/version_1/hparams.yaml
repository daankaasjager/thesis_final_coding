config:
  T: 0
  backbone: dit
  callbacks:
    checkpoint_every_n_steps:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      auto_insert_metric_name: false
      dirpath: /scratch/s3905845/thesis_final_coding/checkpoints
      every_n_train_steps: 4000
      save_last: true
      save_top_k: -1
      verbose: true
    checkpoint_monitor:
      _target_: lightning.pytorch.callbacks.ModelCheckpoint
      auto_insert_metric_name: false
      dirpath: /scratch/s3905845/thesis_final_coding/checkpoints
      filename: best
      mode: min
      monitor: val/nll
      save_last: false
      save_top_k: 1
      verbose: true
    learning_rate_monitor:
      _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: step
  data:
    cache_dir: /scratch/s3905845/thesis_final_coding/data/cache_dir/kraken
    streaming: false
    tokenizer_name_or_path: selfies-wordlevel
    train: kraken-train
    valid: kraken-valid
    wrap: true
  diffusion: absorbing_state
  directory_paths:
    cache_dir: /kraken_cache/data
    images_dir: /scratch/s3905845/thesis_final_coding/data/kraken/images
    pre_processed_data: /scratch/s3905845/thesis_final_coding/data/kraken/preprocessed_data.pt
    raw_data: /scratch/s3905845/thesis_final_coding/data/kraken/descriptors_v3.csv
    sampled_data: /scratch/s3905845/thesis_final_coding/data/kraken/sampled_data/generated_samples.json
    tokenizer: /scratch/s3905845/thesis_final_coding/data/vocab_list/tokenizer
    train_data_encoding: /scratch/s3905845/thesis_final_coding/data/vocab_list/train_data_encoding.pt
  eval:
    checkpoint_path: /scratch/s3905845/thesis_final_coding/checkpoints/best-v1.ckpt
    disable_ema: false
    generate_samples: true
  lr_scheduler:
    _target_: transformers.get_constant_schedule_with_warmup
    num_warmup_steps: 2500
  mode:
    checkpointing:
      fresh_data: true
      resume_ckpt_path: /scratch/s3905845/thesis_final_coding/checkpoints/best-v1.ckpt
      resume_from_ckpt: false
      save_dir: /scratch/s3905845/thesis_final_coding
    loader:
      batch_size: 8
      eval_batch_size: 8
      eval_global_batch_size: 16
      global_batch_size: 16
      num_workers: 4
      pin_memory: true
    name: train
    permitted_selfies_length: 175
    row_limit: 10
    trainer:
      _target_: lightning.Trainer
      accelerator: auto
      accumulate_grad_batches: 1
      devices: 2
      gradient_clip_val: 1.0
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      log_every_n_steps: 10
      max_steps: 1000000
      num_nodes: 1
      num_sanity_val_steps: 1
      precision: '32'
      strategy: auto
      val_check_interval: null
    wandb:
      group: null
      id: reshape_1
      job_type: training
      name: reshape
      notes: testing a selfies model
      project: SELFIES-diffusion
      tags:
      - loglinear
      - kraken-train
      - kraken-valid
  model:
    cond_dim: 128
    dropout: 0.1
    hidden_size: 768
    length: 256
    n_blocks: 12
    n_heads: 12
    name: small
    scale_by_sigma: true
    tie_word_embeddings: false
    type: ddit
  noise:
    sigma_max: 20
    sigma_min: 0.0001
    type: loglinear
  optim:
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-08
    lr: 0.0003
    weight_decay: 0
  parameterization: subs
  permitted_selfies_length: 175
  plot_dist: false
  sampling:
    noise_removal: true
    num_sample_batches: 12
    num_sample_log: 2
    num_strides: 16
    predictor: ddpm_cache
    semi_ar: true
    steps: 128
    stride_length: 4
  seed: 1
  subs_masking: false
  time_conditioning: false
  train_test_split:
    train: 0.8
    valid: 0.2
  training:
    antithetic_sampling: true
    change_of_variables: false
    ema: 0.9999
    importance_sampling: false
    sampling_eps: 0.001
tokenizer: !!python/object:src.tokenizer.SelfiesTokenizer
  SPECIAL_TOKENS_ATTRIBUTES:
  - bos_token
  - eos_token
  - unk_token
  - sep_token
  - pad_token
  - cls_token
  - mask_token
  - additional_special_tokens
  _decode_use_source_tokenizer: false
  _in_target_context_manager: false
  _pad_token_type_id: 0
  _processor_class: null
  _special_tokens_map:
    additional_special_tokens: []
    bos_token: '[BOS]'
    cls_token: '[CLS]'
    eos_token: '[EOS]'
    mask_token: '[MASK]'
    pad_token: '[PAD]'
    sep_token: '[SEP]'
    unk_token: '[UNK]'
  _tokenizer: !!python/object/new:tokenizers.Tokenizer
    args:
    - !!python/object/new:tokenizers.models.Model
      state: !!binary |
        eyJ0eXBlIjoiQlBFIiwiZHJvcG91dCI6bnVsbCwidW5rX3Rva2VuIjpudWxsLCJjb250aW51aW5n
        X3N1YndvcmRfcHJlZml4IjpudWxsLCJlbmRfb2Zfd29yZF9zdWZmaXgiOm51bGwsImZ1c2VfdW5r
        IjpmYWxzZSwiYnl0ZV9mYWxsYmFjayI6ZmFsc2UsImlnbm9yZV9tZXJnZXMiOmZhbHNlLCJ2b2Nh
        YiI6e30sIm1lcmdlcyI6W119
    state: !!binary |
      eyJ2ZXJzaW9uIjoiMS4wIiwidHJ1bmNhdGlvbiI6bnVsbCwicGFkZGluZyI6eyJzdHJhdGVneSI6
      IkJhdGNoTG9uZ2VzdCIsImRpcmVjdGlvbiI6IlJpZ2h0IiwicGFkX3RvX211bHRpcGxlX29mIjpu
      dWxsLCJwYWRfaWQiOjQsInBhZF90eXBlX2lkIjowLCJwYWRfdG9rZW4iOiJbUEFEXSJ9LCJhZGRl
      ZF90b2tlbnMiOlt7ImlkIjowLCJjb250ZW50IjoiW0JPU10iLCJzaW5nbGVfd29yZCI6ZmFsc2Us
      ImxzdHJpcCI6ZmFsc2UsInJzdHJpcCI6ZmFsc2UsIm5vcm1hbGl6ZWQiOmZhbHNlLCJzcGVjaWFs
      Ijp0cnVlfSx7ImlkIjoxLCJjb250ZW50IjoiW0VPU10iLCJzaW5nbGVfd29yZCI6ZmFsc2UsImxz
      dHJpcCI6ZmFsc2UsInJzdHJpcCI6ZmFsc2UsIm5vcm1hbGl6ZWQiOmZhbHNlLCJzcGVjaWFsIjp0
      cnVlfSx7ImlkIjoyLCJjb250ZW50IjoiW1NFUF0iLCJzaW5nbGVfd29yZCI6ZmFsc2UsImxzdHJp
      cCI6ZmFsc2UsInJzdHJpcCI6ZmFsc2UsIm5vcm1hbGl6ZWQiOmZhbHNlLCJzcGVjaWFsIjp0cnVl
      fSx7ImlkIjozLCJjb250ZW50IjoiW0NMU10iLCJzaW5nbGVfd29yZCI6ZmFsc2UsImxzdHJpcCI6
      ZmFsc2UsInJzdHJpcCI6ZmFsc2UsIm5vcm1hbGl6ZWQiOmZhbHNlLCJzcGVjaWFsIjp0cnVlfSx7
      ImlkIjo0LCJjb250ZW50IjoiW1BBRF0iLCJzaW5nbGVfd29yZCI6ZmFsc2UsImxzdHJpcCI6ZmFs
      c2UsInJzdHJpcCI6ZmFsc2UsIm5vcm1hbGl6ZWQiOmZhbHNlLCJzcGVjaWFsIjp0cnVlfSx7Imlk
      Ijo1LCJjb250ZW50IjoiW01BU0tdIiwic2luZ2xlX3dvcmQiOmZhbHNlLCJsc3RyaXAiOmZhbHNl
      LCJyc3RyaXAiOmZhbHNlLCJub3JtYWxpemVkIjpmYWxzZSwic3BlY2lhbCI6dHJ1ZX0seyJpZCI6
      NiwiY29udGVudCI6IltVTktdIiwic2luZ2xlX3dvcmQiOmZhbHNlLCJsc3RyaXAiOmZhbHNlLCJy
      c3RyaXAiOmZhbHNlLCJub3JtYWxpemVkIjpmYWxzZSwic3BlY2lhbCI6dHJ1ZX1dLCJub3JtYWxp
      emVyIjpudWxsLCJwcmVfdG9rZW5pemVyIjpudWxsLCJwb3N0X3Byb2Nlc3NvciI6bnVsbCwiZGVj
      b2RlciI6bnVsbCwibW9kZWwiOnsidHlwZSI6IldvcmRMZXZlbCIsInZvY2FiIjp7IltCT1NdIjow
      LCJbRU9TXSI6MSwiW1NFUF0iOjIsIltDTFNdIjozLCJbUEFEXSI6NCwiW01BU0tdIjo1LCJbVU5L
      XSI6NiwiW0NdIjo3LCJbPUNdIjo4LCJbRl0iOjksIlsjQnJhbmNoMV0iOjEwLCJbQnJhbmNoMl0i
      OjExLCJbPUJyYW5jaDJdIjoxMiwiW05dIjoxMywiW1BdIjoxNCwiW1JpbmcyXSI6MTUsIltSaW5n
      MV0iOjE2LCJbI0JyYW5jaDJdIjoxNywiWy9DXSI6MTgsIls9Tl0iOjE5LCJbQnJhbmNoMV0iOjIw
      LCJbPUJyYW5jaDFdIjoyMSwiW09dIjoyMiwiWyNDXSI6MjN9LCJ1bmtfdG9rZW4iOiJbVU5LXSJ9
      fQ==
  add_prefix_space: false
  chat_template: null
  clean_up_tokenization_spaces: false
  deprecation_warnings: {}
  extra_special_tokens: {}
  init_inputs: !!python/tuple []
  init_kwargs:
    bos_token: '[BOS]'
    cls_token: '[CLS]'
    eos_token: '[EOS]'
    mask_token: '[MASK]'
    pad_token: '[PAD]'
    sep_token: '[SEP]'
    unk_token: '[UNK]'
  model_input_names:
  - input_ids
  - token_type_ids
  - attention_mask
  model_max_length: 1000000000000000019884624838656
  name_or_path: ''
  padding_side: right
  split_special_tokens: false
  truncation_side: right
  verbose: false
  vocab_dict:
    '[#Branch1]': 10
    '[#Branch2]': 17
    '[#C]': 23
    '[/C]': 18
    '[=Branch1]': 21
    '[=Branch2]': 12
    '[=C]': 8
    '[=N]': 19
    '[BOS]': 0
    '[Branch1]': 20
    '[Branch2]': 11
    '[CLS]': 3
    '[C]': 7
    '[EOS]': 1
    '[F]': 9
    '[MASK]': 5
    '[N]': 13
    '[O]': 22
    '[PAD]': 4
    '[P]': 14
    '[Ring1]': 16
    '[Ring2]': 15
    '[SEP]': 2
    '[UNK]': 6
